{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Gede Indra Adi Brata - 225150200111006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 525,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import sys"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 526,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Nilai a (gradien): 9.030907860017422\n",
      "Nilai b (intersep): 2.7848249687518405\n",
      "Final cost: 0.004982193749795959\n",
      "Total epochs run: 1000\n"
     ]
    }
   ],
   "source": [
    "class Adaline:\n",
    "    def __init__(self, learning_rate=0.01, n_epochs=1000, tol=1e-4):\n",
    "        self.learning_rate = learning_rate\n",
    "        self.n_epochs = n_epochs\n",
    "        self.tol = tol\n",
    "        self.weights = None\n",
    "        self.bias = None\n",
    "        self.cost_ = []\n",
    "\n",
    "    def fit(self, X, y):\n",
    "        n_samples, n_features = X.shape\n",
    "        self.weights = np.random.rand(n_features)\n",
    "        self.bias = 0.0\n",
    "        previous_cost = float('inf')\n",
    "\n",
    "        for epoch in range(self.n_epochs):\n",
    "            net_input = np.dot(X, self.weights) + self.bias\n",
    "            output = net_input\n",
    "            errors = y - output\n",
    "            \n",
    "            self.weights += (self.learning_rate / n_samples) * np.dot(X.T, errors)\n",
    "            self.bias += (self.learning_rate / n_samples) * errors.sum()\n",
    "            \n",
    "            cost = (errors ** 2).mean() / 2.0\n",
    "            self.cost_.append(cost)\n",
    "            \n",
    "            if abs(previous_cost - cost) < self.tol:\n",
    "                print(f\"Early stopping at epoch {epoch + 1}\")\n",
    "                break\n",
    "            previous_cost = cost\n",
    "\n",
    "        return self.weights, self.bias, self.cost_\n",
    "\n",
    "    def predict(self, X):\n",
    "        return np.dot(X, self.weights) + self.bias\n",
    "\n",
    "\n",
    "X = np.array([1, 2, 3, 4, 5, 6, 7, 8, 9, 10]).reshape(-1, 1)\n",
    "y = np.array([12, 21, 30, 39, 48, 57, 66, 75, 84, 93])\n",
    "\n",
    "adaline = Adaline(learning_rate=0.01, n_epochs=1000, tol=1e-6)\n",
    "\n",
    "weights, bias, cost_ = adaline.fit(X, y)\n",
    "\n",
    "a = weights[0]\n",
    "b = bias\n",
    "\n",
    "print(f\"Nilai a (gradien): {a}\")\n",
    "print(f\"Nilai b (intersep): {b}\")\n",
    "print(f\"Final cost: {cost_[-1]}\")\n",
    "print(f\"Total epochs run: {len(cost_)}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan data di atas maka nilai a (learning rate) adalah 9 dan b (bias) adalah 3 (setelah dibulatkan)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 527,
   "metadata": {},
   "outputs": [],
   "source": [
    "def weighted_sum(weight, x, bias):\n",
    "  return np.dot(weight, x) + bias\n",
    "\n",
    "def adaline_fit(train, target, epoch, learning_rate, max_stop):\n",
    "  w = np.random.random(len(train[0]))\n",
    "  b = 1\n",
    "\n",
    "  for _ in range(epoch):\n",
    "    max_err = -sys.maxsize\n",
    "    w_new = np.copy(w)\n",
    "    b_new = b \n",
    "    \n",
    "    for r, row in enumerate(train):\n",
    "      y_in = weighted_sum(w, row, b)\n",
    "      error = target[r] - y_in \n",
    "      w_new = [w[i] + learning_rate * error * row[i] for i in range(len(row))]\n",
    "      b_new = b + learning_rate * error  \n",
    "\n",
    "      max_err = max(max_err, abs(target[r] - y_in))\n",
    "    \n",
    "    max_weight_change = max(abs(w_new[i] - w[i]) for i in range(len(w)))\n",
    "    if max_weight_change < max_stop:\n",
    "      break\n",
    "    \n",
    "    w = w_new\n",
    "    b = b_new  \n",
    "\n",
    "  return w, b\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 528,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [9.1179903449495]\n",
      "Bias: 1.8118955849999419\n"
     ]
    }
   ],
   "source": [
    "train_data = np.array([\n",
    "  [1],\n",
    "  [2],\n",
    "  [3],\n",
    "  [4],\n",
    "  [5],\n",
    "  [6],\n",
    "  [7],\n",
    "  [8],\n",
    "  [9],\n",
    "  [10]\n",
    "])\n",
    "\n",
    "target_values = np.array([12,21,30,39,48,57,66,75,84,93])\n",
    "\n",
    "epochs = 1000\n",
    "learning_rate = 0.01\n",
    "max_stop = 0.001\n",
    "\n",
    "weights, bias = adaline_fit(train_data, target_values, epochs, learning_rate, max_stop)\n",
    "\n",
    "print(\"Weights:\", weights)\n",
    "print(\"Bias:\", bias)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Berdasarkan nilai di atas maka nilai dari a adalah 9 (setelah dibulatkan) dan nilai dari bias adalah 2 (setelah dibulatkan)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Disini terdapat inkonsistensi nilai b, saya sudah mencoba mencari namun belum berhasil menemukan, dimana saya merasa logic yang digunakan sudah sama."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
