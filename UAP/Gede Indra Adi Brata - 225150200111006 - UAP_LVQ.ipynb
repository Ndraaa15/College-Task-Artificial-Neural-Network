{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Name : Gede Indra Adi Brata\n",
    "### NIM  : 225150200111006"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.datasets import make_classification"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Tugas"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan modifikasi kode untuk algoritma lain dari LVQ yaitu LVQ2, dan LVQ2.1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVQ"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "\n",
    "class LVQ:\n",
    "    \n",
    "    def __init__(self):\n",
    "        self.weights = np.array([]) \n",
    "        self.labels = np.array([])\n",
    "    \n",
    "    def train(self, training_data, target_labels, learning_rate, decay_factor, max_epochs):\n",
    "        unique_labels, initial_indices = np.unique(target_labels, return_index=True)\n",
    "        self.labels = unique_labels\n",
    "        self.weights = training_data[initial_indices].astype(np.float64)\n",
    "        remaining_data = np.array([sample for i, sample in enumerate(zip(training_data, target_labels)) if i not in initial_indices], dtype='object')\n",
    "        training_data, target_labels = remaining_data[:, 0], remaining_data[:, 1]\n",
    "        \n",
    "        epoch = 0\n",
    "        \n",
    "        while epoch < max_epochs:\n",
    "            for i, sample in enumerate(training_data):\n",
    "                distances = [np.sum((weight - sample) ** 2) for weight in self.weights]\n",
    "                closest_index = np.argmin(distances)\n",
    "                adjustment_sign = 1 if target_labels[i] == unique_labels[closest_index] else -1\n",
    "                self.weights[closest_index] += adjustment_sign * learning_rate * (sample - self.weights[closest_index])\n",
    "            \n",
    "            learning_rate *= decay_factor\n",
    "            epoch += 1\n",
    "        \n",
    "            print(f'Weights: {self.weights}')\n",
    "        \n",
    "    def predict(self, input_data):\n",
    "        centers, labels = self.weights, self.labels\n",
    "        predictions = []\n",
    "        for sample in input_data:\n",
    "            distances = [np.sum((center - sample) ** 2) for center in centers]\n",
    "            predictions.append(labels[np.argmin(distances)])\n",
    "        return predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVQ 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVQ2(LVQ):\n",
    "    def train(self, training_data, target_labels, learning_rate, decay_factor, max_epochs, epsilon=0.3):\n",
    "        unique_labels, initial_indices = np.unique(target_labels, return_index=True)\n",
    "        self.labels = unique_labels\n",
    "        self.weights = training_data[initial_indices].astype(np.float64)\n",
    "        remaining_data = np.array([(sample, label) for i, (sample, label) in enumerate(zip(training_data, target_labels)) if i not in initial_indices], dtype='object')\n",
    "        training_data, target_labels = remaining_data[:, 0], remaining_data[:, 1]\n",
    "        \n",
    "        epoch = 0\n",
    "        while epoch < max_epochs:\n",
    "            for i, sample in enumerate(training_data):\n",
    "                distances = [np.sum((weight - sample) ** 2) for weight in self.weights]\n",
    "                sorted_indices = np.argsort(distances)[:2]\n",
    "                closest_index, second_closest_index = sorted_indices[0], sorted_indices[1]\n",
    "                \n",
    "                ratio = distances[closest_index] / distances[second_closest_index]\n",
    "                \n",
    "                if ratio <= epsilon:\n",
    "                    if target_labels[i] == self.labels[closest_index] and target_labels[i] != self.labels[second_closest_index]:\n",
    "                        self.weights[closest_index] += learning_rate * (sample - self.weights[closest_index])\n",
    "                        self.weights[second_closest_index] -= learning_rate * (sample - self.weights[second_closest_index])\n",
    "\n",
    "            learning_rate *= decay_factor\n",
    "            epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LVQ 2.1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LVQ2_1(LVQ2):\n",
    "    def train(self, training_data, target_labels, learning_rate, decay_factor, max_epochs, epsilon=0.3):\n",
    "        unique_labels, initial_indices = np.unique(target_labels, return_index=True)\n",
    "        self.labels = unique_labels\n",
    "        self.weights = training_data[initial_indices].astype(np.float64)\n",
    "        remaining_data = np.array([sample for i, sample in enumerate(zip(training_data, target_labels)) if i not in initial_indices], dtype='object')\n",
    "        training_samples, target_labels = remaining_data[:, 0], remaining_data[:, 1]\n",
    "        epoch = 0\n",
    "\n",
    "        while epoch < max_epochs:\n",
    "            for i, sample in enumerate(training_samples):\n",
    "                distances = [sum((weight - sample) ** 2) for weight in self.weights]\n",
    "                distances = np.array(distances)\n",
    "                closest_indices = np.argpartition(distances, 1)[:2]\n",
    "                closest_distances = distances[closest_indices]\n",
    "                sorted_closest_indices = closest_indices[np.argsort(closest_distances)]\n",
    "                closest_index = sorted_closest_indices[0]\n",
    "                second_closest_index = sorted_closest_indices[1] \n",
    "\n",
    "                if closest_distances[0] / closest_distances[1] >= (1 - epsilon):\n",
    "                    if unique_labels[closest_index] != target_labels[i]:\n",
    "                        if unique_labels[second_closest_index] == target_labels[i]:\n",
    "                            self.weights[closest_index] -= learning_rate * (sample - self.weights[closest_index])\n",
    "                            self.weights[second_closest_index] += learning_rate * (sample - self.weights[second_closest_index])\n",
    "                    elif unique_labels[second_closest_index] == target_labels[i]:\n",
    "                        self.weights[closest_index] += learning_rate * (sample - self.weights[closest_index])\n",
    "                        self.weights[second_closest_index] += learning_rate * (sample - self.weights[second_closest_index])\n",
    "\n",
    "            learning_rate *= decay_factor\n",
    "            epoch += 1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Percobaan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lakukan training pada setiap model LVQ dengan parameter :\n",
    "- Learning rate 0.5\n",
    "- Decay factor 0.8\n",
    "- Epoch maksimum 100\n",
    "- Epsilon 0.3\n",
    "\n",
    "Hitung akurasi dari LVQ dengan calc_accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def calculate_accuracy(predicted_labels, true_labels):\n",
    "    correct_predictions = [1 if predicted_labels[i] == true_labels[i] else 0 for i in range(len(predicted_labels))]\n",
    "    return sum(correct_predictions) / len(predicted_labels)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gunakan data berikut untuk melakukan percobaan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X, y = make_classification(n_samples=100, n_features=2, n_redundant=0, n_informative=2, n_classes=4, n_clusters_per_class=1)\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weights: [[-1.61874379 -1.60093655]\n",
      " [ 2.25829101 -2.17176077]\n",
      " [-0.70645229  1.24793325]\n",
      " [ 1.01281198  1.61813049]]\n",
      "Weights: [[-1.56084668 -1.53909152]\n",
      " [ 2.12258451 -2.07468848]\n",
      " [-0.80797913  1.37101327]\n",
      " [ 1.02031097  1.66240381]]\n",
      "Weights: [[-1.50894062 -1.49345983]\n",
      " [ 1.99801684 -1.99375503]\n",
      " [-0.88206437  1.41432414]\n",
      " [ 0.93648713  1.57141144]]\n",
      "Weights: [[-1.45288891 -1.43326479]\n",
      " [ 1.81212519 -1.84493875]\n",
      " [-0.97561059  1.4241893 ]\n",
      " [ 0.9782837   1.53190808]]\n",
      "Weights: [[-1.42063506 -1.40948586]\n",
      " [ 1.73316469 -1.80412195]\n",
      " [-1.05835885  1.41832503]\n",
      " [ 1.03118436  1.50775007]]\n",
      "Weights: [[-1.37022253 -1.42422562]\n",
      " [ 1.65063546 -1.71293995]\n",
      " [-1.3367094   1.50493016]\n",
      " [ 1.51985538  1.6681956 ]]\n",
      "Weights: [[-1.46924686 -1.45240524]\n",
      " [ 1.79587764 -1.8237649 ]\n",
      " [-1.22985502  1.47277602]\n",
      " [ 1.11694224  1.43053265]]\n",
      "Weights: [[-1.48867198 -1.45714336]\n",
      " [ 1.76062925 -1.78062082]\n",
      " [-1.16870072  1.42110908]\n",
      " [ 1.09412378  1.34551348]]\n",
      "Weights: [[-1.42820624 -1.32557996]\n",
      " [ 1.73694422 -1.76054091]\n",
      " [-1.17366341  1.32806993]\n",
      " [ 1.10712322  1.31902755]]\n",
      "Weights: [[-1.40400471 -1.27747747]\n",
      " [ 1.71967222 -1.74801713]\n",
      " [-1.18916401  1.2984776 ]\n",
      " [ 1.1260619   1.30953173]]\n",
      "Weights: [[-1.39141998 -1.25500932]\n",
      " [ 1.70672693 -1.73925342]\n",
      " [-1.20556169  1.28958554]\n",
      " [ 1.14415126  1.30619253]]\n",
      "Weights: [[-1.38363427 -1.24241597]\n",
      " [ 1.6968879  -1.73281737]\n",
      " [-1.22031538  1.28813691]\n",
      " [ 1.15982518  1.3053996 ]]\n",
      "Weights: [[-1.37829355 -1.23442547]\n",
      " [ 1.68933744 -1.7279736 ]\n",
      " [-1.23287666  1.28941618]\n",
      " [ 1.17292239  1.30571041]]\n",
      "Weights: [[-1.37440791 -1.22893509]\n",
      " [ 1.68349924 -1.72427358]\n",
      " [-1.24331104  1.29158732]\n",
      " [ 1.1836834   1.30647656]]\n",
      "Weights: [[-1.37148363 -1.22496849]\n",
      " [ 1.67895694 -1.72141815]\n",
      " [-1.25186762  1.29390319]\n",
      " [ 1.19244529  1.30738823]]\n",
      "Weights: [[-1.36923814 -1.22201009]\n",
      " [ 1.6754047  -1.71919778]\n",
      " [-1.25883168  1.29606392]\n",
      " [ 1.19954131  1.30829587]]\n",
      "Weights: [[-1.36749223 -1.21975766]\n",
      " [ 1.6726149  -1.71746116]\n",
      " [-1.24716044  1.29255949]\n",
      " [ 1.22247122  1.31476766]]\n",
      "Weights: [[-1.36612375 -1.218019  ]\n",
      " [ 1.67041623 -1.71609668]\n",
      " [-1.25323239  1.29465024]\n",
      " [ 1.22540462  1.31495271]]\n",
      "Weights: [[-1.36504524 -1.21666422]\n",
      " [ 1.66867846 -1.71502071]\n",
      " [-1.25801531  1.29634883]\n",
      " [ 1.22789137  1.31518259]]\n",
      "Weights: [[-1.36419206 -1.21560157]\n",
      " [ 1.66730176 -1.71416979]\n",
      " [-1.26179828  1.29772345]\n",
      " [ 1.22996334  1.31541623]]\n",
      "Weights: [[-1.36351528 -1.21476407]\n",
      " [ 1.66620904 -1.71349531]\n",
      " [-1.26479901  1.29883282]\n",
      " [ 1.23167042  1.31563371]]\n",
      "Weights: [[-1.36297738 -1.21410172]\n",
      " [ 1.66534038 -1.7129597 ]\n",
      " [-1.26718418  1.2997263 ]\n",
      " [ 1.23306618  1.31582667]]\n",
      "Weights: [[-1.36254923 -1.21357653]\n",
      " [ 1.66464899 -1.71253374]\n",
      " [-1.26908295  1.30044482]\n",
      " [ 1.23420127  1.31599288]]\n",
      "Weights: [[-1.36220806 -1.21315926]\n",
      " [ 1.66409813 -1.71219457]\n",
      " [-1.27059621  1.30102197]\n",
      " [ 1.23512079  1.31613329]]\n",
      "Weights: [[-1.36193596 -1.21282725]\n",
      " [ 1.66365888 -1.71192427]\n",
      " [-1.27180326  1.30148517]\n",
      " [ 1.23586354  1.3162503 ]]\n",
      "Weights: [[-1.36171881 -1.21256277]\n",
      " [ 1.66330841 -1.71170868]\n",
      " [-1.27276669  1.30185666]\n",
      " [ 1.23646221  1.31634688]]\n",
      "Weights: [[-1.36154542 -1.2123519 ]\n",
      " [ 1.66302862 -1.71153662]\n",
      " [-1.27353603  1.30215445]\n",
      " [ 1.23694397  1.31642602]]\n",
      "Weights: [[-1.36140692 -1.21218365]\n",
      " [ 1.66280516 -1.71139925]\n",
      " [-1.27415064  1.30239305]\n",
      " [ 1.23733115  1.31649053]]\n",
      "Weights: [[-1.36129625 -1.21204933]\n",
      " [ 1.66262664 -1.71128952]\n",
      " [-1.27464177  1.30258417]\n",
      " [ 1.23764202  1.3165429 ]]\n",
      "Weights: [[-1.3612078  -1.21194206]\n",
      " [ 1.66248398 -1.71120184]\n",
      " [-1.27503433  1.30273722]\n",
      " [ 1.23789143  1.31658528]]\n",
      "Weights: [[-1.3611371  -1.21185635]\n",
      " [ 1.66236995 -1.71113177]\n",
      " [-1.27534815  1.30285976]\n",
      " [ 1.23809141  1.31661949]]\n",
      "Weights: [[-1.36108057 -1.21178786]\n",
      " [ 1.66227879 -1.71107576]\n",
      " [-1.27559907  1.30295785]\n",
      " [ 1.23825169  1.31664705]]\n",
      "Weights: [[-1.36103537 -1.21173311]\n",
      " [ 1.6622059  -1.71103098]\n",
      " [-1.27579972  1.30303636]\n",
      " [ 1.23838009  1.31666923]]\n",
      "Weights: [[-1.36099922 -1.21168935]\n",
      " [ 1.66214761 -1.71099517]\n",
      " [-1.27596017  1.3030992 ]\n",
      " [ 1.23848293  1.31668706]]\n",
      "Weights: [[-1.36097031 -1.21165435]\n",
      " [ 1.662101   -1.71096654]\n",
      " [-1.27608851  1.30314948]\n",
      " [ 1.23856528  1.31670137]]\n",
      "Weights: [[-1.36094719 -1.21162637]\n",
      " [ 1.66206372 -1.71094364]\n",
      " [-1.27619115  1.30318972]\n",
      " [ 1.23863121  1.31671285]]\n",
      "Weights: [[-1.36092869 -1.21160399]\n",
      " [ 1.66203391 -1.71092532]\n",
      " [-1.27627325  1.30322191]\n",
      " [ 1.23868398  1.31672205]]\n",
      "Weights: [[-1.3609139  -1.21158609]\n",
      " [ 1.66201006 -1.71091067]\n",
      " [-1.27633892  1.30324768]\n",
      " [ 1.23872622  1.31672943]]\n",
      "Weights: [[-1.36090207 -1.21157177]\n",
      " [ 1.66199098 -1.71089896]\n",
      " [-1.27639144  1.30326829]\n",
      " [ 1.23876002  1.31673534]]\n",
      "Weights: [[-1.3608926  -1.21156032]\n",
      " [ 1.66197572 -1.71088958]\n",
      " [-1.27643346  1.30328478]\n",
      " [ 1.23878707  1.31674007]]\n",
      "Weights: [[-1.36088503 -1.21155116]\n",
      " [ 1.66196352 -1.71088209]\n",
      " [-1.27646708  1.30329797]\n",
      " [ 1.23880871  1.31674387]]\n",
      "Weights: [[-1.36087897 -1.21154384]\n",
      " [ 1.66195375 -1.71087609]\n",
      " [-1.27649397  1.30330853]\n",
      " [ 1.23882603  1.3167469 ]]\n",
      "Weights: [[-1.36087413 -1.21153798]\n",
      " [ 1.66194594 -1.71087129]\n",
      " [-1.27651548  1.30331697]\n",
      " [ 1.23883989  1.31674933]]\n",
      "Weights: [[-1.36087025 -1.21153329]\n",
      " [ 1.6619397  -1.71086746]\n",
      " [-1.27653268  1.30332373]\n",
      " [ 1.23885098  1.31675127]]\n",
      "Weights: [[-1.36086715 -1.21152954]\n",
      " [ 1.6619347  -1.71086439]\n",
      " [-1.27654645  1.30332914]\n",
      " [ 1.23885985  1.31675283]]\n",
      "Weights: [[-1.36086467 -1.21152654]\n",
      " [ 1.6619307  -1.71086193]\n",
      " [-1.27655746  1.30333346]\n",
      " [ 1.23886695  1.31675408]]\n",
      "Weights: [[-1.36086269 -1.21152414]\n",
      " [ 1.6619275  -1.71085996]\n",
      " [-1.27656627  1.30333692]\n",
      " [ 1.23887262  1.31675507]]\n",
      "Weights: [[-1.3608611  -1.21152222]\n",
      " [ 1.66192494 -1.71085839]\n",
      " [-1.27657332  1.30333969]\n",
      " [ 1.23887717  1.31675587]]\n",
      "Weights: [[-1.36085983 -1.21152068]\n",
      " [ 1.66192289 -1.71085714]\n",
      " [-1.27657896  1.3033419 ]\n",
      " [ 1.2388808   1.31675651]]\n",
      "Weights: [[-1.36085881 -1.21151945]\n",
      " [ 1.66192126 -1.71085613]\n",
      " [-1.27658347  1.30334367]\n",
      " [ 1.23888371  1.31675702]]\n",
      "Weights: [[-1.360858   -1.21151847]\n",
      " [ 1.66191995 -1.71085532]\n",
      " [-1.27658708  1.30334509]\n",
      " [ 1.23888603  1.31675743]]\n",
      "Weights: [[-1.36085735 -1.21151768]\n",
      " [ 1.6619189  -1.71085468]\n",
      " [-1.27658996  1.30334622]\n",
      " [ 1.2388879   1.31675775]]\n",
      "Weights: [[-1.36085683 -1.21151705]\n",
      " [ 1.66191806 -1.71085417]\n",
      " [-1.27659227  1.30334713]\n",
      " [ 1.23888938  1.31675801]]\n",
      "Weights: [[-1.36085642 -1.21151655]\n",
      " [ 1.66191739 -1.71085375]\n",
      " [-1.27659412  1.30334786]\n",
      " [ 1.23889057  1.31675822]]\n",
      "Weights: [[-1.36085608 -1.21151615]\n",
      " [ 1.66191685 -1.71085342]\n",
      " [-1.2765956   1.30334844]\n",
      " [ 1.23889153  1.31675839]]\n",
      "Weights: [[-1.36085582 -1.21151583]\n",
      " [ 1.66191642 -1.71085316]\n",
      " [-1.27659678  1.3033489 ]\n",
      " [ 1.23889229  1.31675852]]\n",
      "Weights: [[-1.3608556  -1.21151557]\n",
      " [ 1.66191608 -1.71085295]\n",
      " [-1.27659772  1.30334927]\n",
      " [ 1.2388929   1.31675863]]\n",
      "Weights: [[-1.36085543 -1.21151536]\n",
      " [ 1.6619158  -1.71085278]\n",
      " [-1.27659848  1.30334957]\n",
      " [ 1.23889339  1.31675872]]\n",
      "Weights: [[-1.3608553  -1.2115152 ]\n",
      " [ 1.66191558 -1.71085265]\n",
      " [-1.27659909  1.30334981]\n",
      " [ 1.23889378  1.31675879]]\n",
      "Weights: [[-1.36085519 -1.21151506]\n",
      " [ 1.66191541 -1.71085254]\n",
      " [-1.27659957  1.30335   ]\n",
      " [ 1.23889409  1.31675884]]\n",
      "Weights: [[-1.3608551  -1.21151496]\n",
      " [ 1.66191527 -1.71085245]\n",
      " [-1.27659996  1.30335015]\n",
      " [ 1.23889434  1.31675888]]\n",
      "Weights: [[-1.36085503 -1.21151487]\n",
      " [ 1.66191515 -1.71085238]\n",
      " [-1.27660027  1.30335027]\n",
      " [ 1.23889454  1.31675892]]\n",
      "Weights: [[-1.36085497 -1.21151481]\n",
      " [ 1.66191506 -1.71085233]\n",
      " [-1.27660052  1.30335037]\n",
      " [ 1.2388947   1.31675895]]\n",
      "Weights: [[-1.36085493 -1.21151475]\n",
      " [ 1.66191499 -1.71085228]\n",
      " [-1.27660071  1.30335045]\n",
      " [ 1.23889483  1.31675897]]\n",
      "Weights: [[-1.36085489 -1.21151471]\n",
      " [ 1.66191494 -1.71085225]\n",
      " [-1.27660087  1.30335051]\n",
      " [ 1.23889493  1.31675899]]\n",
      "Weights: [[-1.36085487 -1.21151468]\n",
      " [ 1.66191489 -1.71085222]\n",
      " [-1.276601    1.30335056]\n",
      " [ 1.23889501  1.316759  ]]\n",
      "Weights: [[-1.36085484 -1.21151465]\n",
      " [ 1.66191485 -1.7108522 ]\n",
      " [-1.2766011   1.3033506 ]\n",
      " [ 1.23889508  1.31675901]]\n",
      "Weights: [[-1.36085482 -1.21151463]\n",
      " [ 1.66191482 -1.71085218]\n",
      " [-1.27660118  1.30335063]\n",
      " [ 1.23889513  1.31675902]]\n",
      "Weights: [[-1.36085481 -1.21151461]\n",
      " [ 1.6619148  -1.71085216]\n",
      " [-1.27660125  1.30335066]\n",
      " [ 1.23889517  1.31675903]]\n",
      "Weights: [[-1.3608548  -1.21151459]\n",
      " [ 1.66191478 -1.71085215]\n",
      " [-1.2766013   1.30335068]\n",
      " [ 1.2388952   1.31675904]]\n",
      "Weights: [[-1.36085479 -1.21151458]\n",
      " [ 1.66191476 -1.71085214]\n",
      " [-1.27660134  1.30335069]\n",
      " [ 1.23889523  1.31675904]]\n",
      "Weights: [[-1.36085478 -1.21151457]\n",
      " [ 1.66191475 -1.71085214]\n",
      " [-1.27660138  1.30335071]\n",
      " [ 1.23889525  1.31675904]]\n",
      "Weights: [[-1.36085478 -1.21151457]\n",
      " [ 1.66191474 -1.71085213]\n",
      " [-1.2766014   1.30335072]\n",
      " [ 1.23889527  1.31675905]]\n",
      "Weights: [[-1.36085477 -1.21151456]\n",
      " [ 1.66191474 -1.71085213]\n",
      " [-1.27660142  1.30335072]\n",
      " [ 1.23889528  1.31675905]]\n",
      "Weights: [[-1.36085477 -1.21151456]\n",
      " [ 1.66191473 -1.71085212]\n",
      " [-1.27660144  1.30335073]\n",
      " [ 1.2388953   1.31675905]]\n",
      "Weights: [[-1.36085476 -1.21151455]\n",
      " [ 1.66191472 -1.71085212]\n",
      " [-1.27660145  1.30335074]\n",
      " [ 1.2388953   1.31675905]]\n",
      "Weights: [[-1.36085476 -1.21151455]\n",
      " [ 1.66191472 -1.71085212]\n",
      " [-1.27660146  1.30335074]\n",
      " [ 1.23889531  1.31675905]]\n",
      "Weights: [[-1.36085476 -1.21151455]\n",
      " [ 1.66191472 -1.71085211]\n",
      " [-1.27660147  1.30335074]\n",
      " [ 1.23889532  1.31675906]]\n",
      "Weights: [[-1.36085476 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660148  1.30335075]\n",
      " [ 1.23889532  1.31675906]]\n",
      "Weights: [[-1.36085476 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660149  1.30335075]\n",
      " [ 1.23889532  1.31675906]]\n",
      "Weights: [[-1.36085476 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660149  1.30335075]\n",
      " [ 1.23889533  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660149  1.30335075]\n",
      " [ 1.23889533  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.2766015   1.30335075]\n",
      " [ 1.23889533  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.2766015   1.30335075]\n",
      " [ 1.23889533  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.2766015   1.30335076]\n",
      " [ 1.23889533  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.2766015   1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.2766015   1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.2766015   1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.66191471 -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Weights: [[-1.36085475 -1.21151454]\n",
      " [ 1.6619147  -1.71085211]\n",
      " [-1.27660151  1.30335076]\n",
      " [ 1.23889534  1.31675906]]\n",
      "Accuracy LVQ1: 0.9\n",
      "Accuracy LVQ2: 0.15\n",
      "Accuracy LVQ2.1: 0.75\n"
     ]
    }
   ],
   "source": [
    "learning_rate = 0.5\n",
    "decay_factor = 0.8\n",
    "max_epochs = 100\n",
    "epsilon = 0.3\n",
    "\n",
    "# Train and test LVQ1\n",
    "lvq1 = LVQ()\n",
    "lvq1.train(X_train, y_train, learning_rate, decay_factor, max_epochs)\n",
    "predictions_lvq1 = lvq1.predict(X_test)\n",
    "accuracy_lvq1 = calculate_accuracy(predictions_lvq1, y_test)\n",
    "print(f'Accuracy LVQ1: {accuracy_lvq1}')\n",
    "\n",
    "# Train and test LVQ2\n",
    "lvq2 = LVQ2()\n",
    "lvq2.train(X_train, y_train, learning_rate, decay_factor, max_epochs, epsilon=epsilon)\n",
    "predictions_lvq2 = lvq2.predict(X_test)\n",
    "accuracy_lvq2 = calculate_accuracy(predictions_lvq2, y_test)\n",
    "print(f'Accuracy LVQ2: {accuracy_lvq2}')\n",
    "\n",
    "# Train and test LVQ2.1\n",
    "lvq2_1 = LVQ2_1()\n",
    "lvq2_1.train(X_train, y_train, learning_rate, decay_factor, max_epochs, epsilon=epsilon)\n",
    "predictions_lvq2_1 = lvq2_1.predict(X_test)\n",
    "accuracy_lvq2_1 = calculate_accuracy(predictions_lvq2_1, y_test)\n",
    "print(f'Accuracy LVQ2.1: {accuracy_lvq2_1}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Analisis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Jika terdapat perbedaan performa pada setiap model LVQ, mengapa hal itu dapat terjadi?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perbedaan performa antara model LVQ1, LVQ2, dan LVQ2.1 disebabkan oleh perbedaan algoritma pembaruan bobot serta kondisi pembelajaran yang diterapkan pada masing-masing model. LVQ1 menggunakan pendekatan dasar dengan memperbarui bobot hanya berdasarkan hubungan antara sampel data dengan vektor bobot terdekatnya, tanpa mempertimbangkan konteks yang lebih kompleks seperti rasio jarak atau hubungan antar kelas. Pada algoritma LVQ2 memperkenalkan konsep rasio jarak (distance ratio) dengan parameter epsilon untuk menentukan apakah pembaruan bobot dilakukan, serta memperhitungkan vektor bobot kedua terdekat, sehingga mampu menangkap lebih banyak informasi mengenai distribusi data, terutama pada data yang saling tumpang tindih (overlapping). Pada LVQ2.1  menerapkan kondisi tambahan terkait label target dan rasio jarak untuk meningkatkan akurasi dalam skenario di mana data memiliki pola yang lebih kompleks. Perbedaan ini membuat setiap model memiliki tingkat sensitivitas yang berbeda terhadap struktur data, distribusi kelas, dan tingkat noise, sehingga menghasilkan performa yang bervariasi tergantung pada karakteristik dataset yang digunakan."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "2. Jelaskan resiko penggunaan Euclidean Distance sebagai pengukur jarak weight dengan komponen masukan vektor (X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Penggunaan Euclidean Distance sebagai pengukur jarak antara bobot (weight) dengan komponen vektor masukan (X) dalam LVQ memiliki beberapa risiko. Salah satu risiko utama adalah sensitivitas Euclidean Distance terhadap skala data. Jika fitur dalam dataset memiliki skala yang berbeda, fitur dengan nilai lebih besar akan mendominasi perhitungan jarak, sehingga mengaburkan kontribusi fitur dengan nilai lebih kecil. Hal ini dapat menyebabkan model LVQ memberikan pembaruan bobot yang bias terhadap dimensi tertentu, mengurangi efektivitas proses pembelajaran. Selain itu, Euclidean Distance mengasumsikan bahwa semua fitur bersifat independen dan memiliki hubungan linier, yang tidak selalu sesuai dengan data dunia nyata. Pada dataset dengan hubungan non-linier antar fitur, pengukuran jarak ini dapat menghasilkan representasi yang kurang akurat. Risiko lainnya adalah sensitivitas terhadap outlier; nilai ekstrem dalam data dapat memperbesar jarak secara signifikan, sehingga mempengaruhi arah pembaruan bobot. Oleh karena itu, penting untuk memastikan bahwa data telah dinormalisasi dan relevansi fitur dievaluasi sebelum menggunakan Euclidean Distance dalam LVQ."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
